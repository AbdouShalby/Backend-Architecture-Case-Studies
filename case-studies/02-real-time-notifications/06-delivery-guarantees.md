# 6. Delivery Guarantees

> "Best effort" is fine for "someone liked your post." It is NOT fine for "your 2FA code is 847291." Understanding the difference is what separates a notification service from a notification *system*.

---

## ğŸ¯ Delivery Tiers

| Tier | Guarantee | Retry | Example | SLA |
|------|-----------|-------|---------|-----|
| **Critical** | Must deliver, all channels | Infinite retry + SMS fallback | 2FA code, payment alert | 99.99% delivered in 30s |
| **High** | Should deliver, primary channel + fallback | 3 retries over 15 min | Order confirmation, shipping update | 99.9% delivered in 5 min |
| **Normal** | Best effort with retry | 2 retries over 1 hour | New follower, message received | 99% delivered in 1 hour |
| **Low** | Best effort, no retry | No retry | Marketing promo, weekly digest | 95% delivered same day |

### Priority Routing

```mermaid
graph TB
    N["Incoming Notification"] --> P{"Priority?"}

    P -->|Critical| C["ğŸ”´ Critical Path<br/>â€¢ Dedicated workers<br/>â€¢ Separate queue<br/>â€¢ Multi-channel blast<br/>â€¢ Infinite retry"]

    P -->|High| H["ğŸŸ  High Priority<br/>â€¢ Priority queue position<br/>â€¢ 3 retries / 15 min<br/>â€¢ Fallback channel"]

    P -->|Normal| NR["ğŸŸ¡ Normal<br/>â€¢ Standard queue<br/>â€¢ 2 retries / 1 hour<br/>â€¢ Single channel"]

    P -->|Low| L["ğŸŸ¢ Low Priority<br/>â€¢ Background queue<br/>â€¢ No retry<br/>â€¢ Rate limited"]

    style C fill:#f44336,color:#fff
    style H fill:#ff9800,color:#fff
    style NR fill:#ffeb3b,color:#000
    style L fill:#4caf50,color:#fff
```

---

## ğŸ”´ Critical Notification Path (2FA, Payment)

```mermaid
sequenceDiagram
    participant SRC as Auth Service
    participant API as Notification API
    participant CQ as Critical Queue
    participant W as Critical Worker
    participant SMS as Twilio
    participant PUSH as FCM
    participant EMAIL as SES
    participant DB as Delivery Log

    SRC->>API: Send 2FA code (priority: CRITICAL)
    API->>CQ: Produce to critical queue (dedicated topic)

    CQ->>W: Consume (dedicated worker pool)

    par Channel 1: SMS (primary for 2FA)
        W->>SMS: Send SMS
        SMS-->>W: Delivered âœ…
        W->>DB: Log: SMS delivered
    and Channel 2: Push (backup)
        W->>PUSH: Send push
        PUSH-->>W: Sent âœ…
        W->>DB: Log: Push sent
    end

    Note over W,SMS: If SMS fails:
    W->>SMS: Retry 1 (after 5s)
    SMS--xW: Failed
    W->>SMS: Retry 2 (after 15s)
    SMS-->>W: Delivered âœ…
    W->>DB: Log: SMS delivered (attempt 3)
```

### Critical Path Isolation

```
Why separate everything for critical notifications?

1. Separate Kafka topic: notifications.critical
   â†’ Not competing with 200M marketing/social notifications
   â†’ Guaranteed partition availability

2. Dedicated worker pool: 3-5 workers (always running)
   â†’ Not affected by flash sale traffic surge
   â†’ Reserved capacity

3. Separate rate limit: No rate limiting for critical notifications
   â†’ 2FA code ALWAYS gets through
   â†’ Override any user/global throttle

4. Multi-channel blast: Send via ALL channels simultaneously
   â†’ Don't wait for SMS to fail before trying push
   â†’ Whichever arrives first wins

5. Aggressive retry: 
   â†’ Retry every 5s, 15s, 30s, 60s, 120s, 300s
   â†’ Alert ops if still failing after 5 minutes
   â†’ Never give up on critical (manual escalation)
```

---

## ğŸ” Retry Strategy

### Retry Configuration Per Priority

| Priority | Max Attempts | Backoff Schedule | DLQ After |
|----------|-------------|------------------|-----------|
| Critical | âˆ (alert at 10) | 5s, 15s, 30s, 60s, 120s, 300s, repeat | Never (manual) |
| High | 5 | 10s, 30s, 2min, 10min, 30min | 5th failure |
| Normal | 3 | 1min, 15min, 1h | 3rd failure |
| Low | 1 | No retry | Immediate |

### Retry Implementation (Kafka-based)

```
Retry queues using Kafka topics with delay:

notifications.in_app           â† main queue
notifications.in_app.retry_1   â† 10-second delay (consumer lag intentional)
notifications.in_app.retry_2   â† 2-minute delay
notifications.in_app.retry_3   â† 15-minute delay
notifications.in_app.dlq       â† dead letter

Flow:
  1. Consume from main queue â†’ attempt delivery
  2. If failed â†’ produce to retry_1 (with attempt count in header)
  3. Retry_1 consumer waits 10s â†’ attempts delivery
  4. If failed â†’ produce to retry_2
  5. After all retries exhausted â†’ produce to DLQ
```

### Channel Fallback Strategy

```
For HIGH priority notifications:

Primary channel fails (e.g., push) â†’ try fallback channel (e.g., email)

Fallback chain:
  In-App â†’ Push â†’ Email â†’ SMS (last resort)

Example: Order shipped notification
  Attempt 1: In-App (WebSocket) â†’ user offline â†’  MISS
  Attempt 2: Push (FCM)        â†’ token invalid â†’ FAIL
  Attempt 3: Email (SES)       â†’ sent          â†’ âœ… DELIVERED

Note: SMS is NEVER a fallback for non-critical. Too expensive.
```

---

## ğŸ”‘ Deduplication

### The Problem

```
Scenario: At-least-once delivery means duplicates are possible
  1. Worker sends push notification
  2. FCM returns 200 âœ…
  3. Worker crashes BEFORE recording "delivered" in DB
  4. Worker restarts, reprocesses same message
  5. User gets the same notification twice

Also: Kafka consumer processes message, crashes before committing offset
  â†’ Same message reprocessed
```

### Deduplication Strategy

```mermaid
graph TB
    M["Message arrives<br/>(notification_id + channel)"] --> C{"Seen before?"}

    C -->|"Yes (Redis)"| SKIP["Skip processing<br/>(idempotent)"]
    C -->|"No"| PROCESS["Process notification"]

    PROCESS --> RECORD["Record in Redis<br/>SETEX dedup:{notif_id}:{channel} 1 86400"]
    PROCESS --> DELIVER["Deliver to channel"]

    style SKIP fill:#4caf50,color:#fff
    style RECORD fill:#dc382d,color:#fff
```

```
Dedup key: dedup:{notification_id}:{channel}:{user_id}
TTL: 24 hours

Before processing:
  exists = Redis GET dedup:notif_xyz:push:usr_42
  if exists â†’ skip (already processed)
  
After processing:
  Redis SETEX dedup:notif_xyz:push:usr_42 1 86400
```

> **âš ï¸ Known Risk: Redis Data Loss Breaks Deduplication**
>
> Dedup keys are stored in Redis with 24-hour TTL. If Redis loses data â€” through memory pressure eviction (if `maxmemory-policy` isn't `noeviction`), node restart, or failover with incomplete replication â€” **dedup keys are lost and duplicate notifications will be delivered**. This means a Redis failure doesn't just cause missed notifications; it can cause duplicate ones.
>
> **Mitigations:**
> - Use `maxmemory-policy: noeviction` for the dedup Redis instance (reject new writes rather than evict)
> - Enable AOF persistence with `appendfsync everysec` (lose at most 1 second of dedup keys on crash)
> - For critical notifications (payment confirmations, security alerts), store dedup keys in Cassandra as the source of truth, with Redis as a fast-path cache
> - Accept that non-critical notification duplicates (social, marketing) are annoying but not harmful

### Idempotency at API Level

```
POST /notifications
Idempotency-Key: "order_shipped_ord_abc123"

Server:
  1. Check: Redis GET idemp:order_shipped_ord_abc123
  2. If exists â†’ return cached response (202 with same notification_id)
  3. If not â†’ process, store response, return 202
  4. SETEX idemp:order_shipped_ord_abc123 {response} 86400

Prevents: duplicate notifications from retrying source services
```

---

## ğŸš¦ Rate Limiting

### Per-User Rate Limits

```
Problem: Bug in social service sends 1000 "new follower" events in 1 second
  â†’ User gets 1000 push notifications â†’ uninstalls app

Per-user rate limits:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Channel                 â”‚ Limit          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ In-App                  â”‚ 100/hour       â”‚
  â”‚ Push                    â”‚ 20/hour        â”‚
  â”‚ Email (transactional)   â”‚ 10/hour        â”‚
  â”‚ Email (marketing)       â”‚ 2/day          â”‚
  â”‚ SMS                     â”‚ 5/hour         â”‚
  â”‚ SMS (2FA)               â”‚ 10/hour        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Exception: Critical notifications bypass ALL limits
```

### Implementation (Redis Sliding Window)

```
-- Check: can user_42 receive another push notification?

Key: rl:push:usr_42:{hour_bucket}
Limit: 20 per hour

INCR rl:push:usr_42:2026022314
EXPIRE rl:push:usr_42:2026022314 7200  -- 2 hours (overlap buffer)

current_count = GET rl:push:usr_42:2026022314
if current_count > 20:
    â†’ SKIP this notification
    â†’ Log: "rate limited, notification dropped"
```

### Global Rate Limits (Provider Throttling)

| Provider | Rate Limit | Our Limit (Safety Margin) |
|----------|-----------|--------------------------|
| FCM | ~1,000 msg/sec default (can request increase) | 800 msg/sec |
| APNs | ~5,000 msg/sec | 4,000 msg/sec |
| Amazon SES | 200 emails/sec (adjustable) | 150 emails/sec |
| Twilio SMS | 300 msg/sec per number | 250 msg/sec |

```
Global rate limiter (token bucket in Redis):
  Each worker acquires a "token" before sending
  If no tokens available â†’ wait or queue

  Prevents: hitting provider rate limits â†’ getting all traffic blocked
```

---

## ğŸ“Š Delivery Status Tracking

### Status Flow Per Channel

```
In-App:  pending â†’ sent â†’ delivered â†’ read
Push:    pending â†’ sent â†’ delivered (FCM callback) â†’ [opened]
Email:   pending â†’ sent â†’ delivered (SES webhook) â†’ [opened] â†’ [clicked]
SMS:     pending â†’ sent â†’ delivered (Twilio callback)
```

### Delivery Metrics

| Metric | In-App | Push | Email | SMS |
|--------|--------|------|-------|-----|
| Sent rate | 99.9% | 98% | 99% | 99.5% |
| Delivered rate | 99% (online users) | 85% | 95% | 97% |
| Read/Open rate | 40% | 8-12% | 20-25% | 90% |
| Avg delivery time | < 500ms | 1-5s | 5-30s | 3-10s |

---

## â¬…ï¸ [â† Fan-Out Strategy](05-fan-out-strategy.md) Â· [Failure & Recovery â†’](07-failure-recovery.md)
